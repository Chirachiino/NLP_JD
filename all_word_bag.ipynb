{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - smail.nju.edu.cn\\Ms@Cu\\BA\\Project\\all_word_bag.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20smail.nju.edu.cn/Ms%40Cu/BA/Project/all_word_bag.ipynb#ch0000001?line=8'>9</a>\u001b[0m jd_data\u001b[39m=\u001b[39mjd_data\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20smail.nju.edu.cn/Ms%40Cu/BA/Project/all_word_bag.ipynb#ch0000001?line=9'>10</a>\u001b[0m corpus\u001b[39m=\u001b[39mjd_data[\u001b[39m'\u001b[39m\u001b[39mJob Description\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20smail.nju.edu.cn/Ms%40Cu/BA/Project/all_word_bag.ipynb#ch0000001?line=10'>11</a>\u001b[0m vectorizer\u001b[39m=\u001b[39mCountVectorizer(ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m),stop_words\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(corpus)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1266'>1267</a>\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1267'>1268</a>\u001b[0m \n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1268'>1269</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1279'>1280</a>\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1280'>1281</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1281'>1282</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1282'>1283</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1283'>1284</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1203\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m         feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1203'>1204</a>\u001b[0m         \u001b[39mif\u001b[39;00m feature_idx \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m feature_counter:\n\u001b[0;32m   <a href='file:///d%3A/ProgramData/Anaconda3/lib/site-packages/sklearn/feature_extraction/text.py?line=1204'>1205</a>\u001b[0m             feature_counter[feature_idx] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "type_names=['DataAnalyst','DataScientist','BusinessAnalyst','DataEngineer']\n",
    "jd_data=pd.DataFrame(columns=(['Job Description','type_name']+type_names))\n",
    "for each in type_names:\n",
    "    jd=pd.read_csv(\"data/generated/{}.csv\".format(each))\n",
    "    jd_cleaned=list(jd['Cleaned JD'].dropna().reset_index(drop=True))\n",
    "\n",
    "    jd_data=pd.concat([jd_data,pd.DataFrame({'Job Description':jd_cleaned,each:1,'type_name':each})])\n",
    "\n",
    "jd_data=jd_data.fillna(0)\n",
    "corpus=jd_data['Job Description']\n",
    "vectorizer=CountVectorizer(ngram_range=(1, 2),stop_words='english').fit(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760557"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_vectors=vectorizer.transform(jd_data['Job Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "    min_samples_split=0.01,\n",
    "    bootstrap=True,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10,\n",
    "    min_samples_split=0.01,\n",
    "    bootstrap=True,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result=rf.fit(jd_vectors,jd_data['type_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances =rf_result.feature_importances_\n",
    "forest_importances = pd.Series(importances,index=vectorizer.get_feature_names_out())\n",
    "forest_importances=forest_importances.sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
